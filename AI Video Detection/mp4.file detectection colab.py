# -*- coding: utf-8 -*-
"""mp4.file detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RuT8O5q1-9n2O-A8iKth_jQNnzeSR1_r
"""

!git clone https://github.com/ultralytics/yolov5.git

# Commented out IPython magic to ensure Python compatibility.
# %cd yolov5

!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117
!pip install yolov5
!pip install transformers
!pip install pillow
!pip install matplotlib

!pip install gradio

!pip install transformers
!pip install torchvision

# Complete single-cell code: Upload, detect, display, download

from google.colab import files
import os
import glob
from moviepy.editor import VideoFileClip
from IPython.display import Video, display
from ultralytics import YOLO

print("Loading YOLOv8 model...")
model = YOLO('yolov8n.pt')  # Nano model for fast inference

print("Please upload your video file (recommended: mp4).")
uploaded = files.upload()

if not uploaded:
    print("No video uploaded. Please run the cell again and upload a video.")
else:
    video_filename = list(uploaded.keys())[0]
    print(f"Uploaded: {video_filename}")

    # Step: Extract subclip from 10s to 20s or video length (whichever shorter)
    start_time = 10
    end_time = 20

    print("Extracting subclip from 10s to 20s (or shorter if video is shorter)...")
    with VideoFileClip(video_filename) as clip:
        if end_time > clip.duration:
            end_time = clip.duration
        if start_time >= end_time:
            print("Invalid subclip times. Using full video instead.")
            subclip_filename = video_filename
        else:
            subclip = clip.subclip(start_time, end_time)
            subclip_filename = f"subclip_{video_filename}"
            subclip.write_videofile(subclip_filename, codec="libx264", audio_codec="aac", logger=None)
            print(f"Subclip saved as {subclip_filename}")

    # Step: Run YOLO detection on subclip
    print("Running YOLOv8 object detection on subclip...")
    results = model.predict(source=subclip_filename, save=True, conf=0.25, imgsz=320, device='cpu', verbose=False)

    # Find output video file (Ultralytics saves in runs/detect/predict*)
    processed_video_base_name = os.path.basename(subclip_filename).rsplit('.',1)[0]
    output_dirs = sorted(glob.glob('runs/detect/predict*'))
    ultralytics_output_video_path = None

    if output_dirs:
        latest_dir = output_dirs[-1]
        candidates = glob.glob(os.path.join(latest_dir, f"{processed_video_base_name}.*"))
        for f in candidates:
            if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.webm')):
                ultralytics_output_video_path = f
                break

    if ultralytics_output_video_path and os.path.exists(ultralytics_output_video_path):
        print(f"Detected video found: {ultralytics_output_video_path}")

        try:
            print("Adding original audio from subclip to detected video...")
            original_clip = VideoFileClip(subclip_filename)
            audio_clip = original_clip.audio
            detected_clip = VideoFileClip(ultralytics_output_video_path)

            final_output = f"final_with_audio_{processed_video_base_name}.mp4"
            if audio_clip:
                final_clip = detected_clip.set_audio(audio_clip)
                final_clip.write_videofile(final_output, codec="libx264", audio_codec="aac", logger=None)
                print(f"Final video with audio saved as: {final_output}")
                display(Video(final_output, embed=True, html_attributes="controls"))
            else:
                print("No audio found in subclip. Displaying detected video without audio.")
                display(Video(ultralytics_output_video_path, embed=True, html_attributes="controls"))
                final_output = ultralytics_output_video_path

        except Exception as e:
            print(f"Error adding audio: {e}. Showing detected video without audio.")
            display(Video(ultralytics_output_video_path, embed=True, html_attributes="controls"))
            final_output = ultralytics_output_video_path

        # Provide download link
        print("\nClick below to download the processed video:")
        files.download(final_output)

    else:
        print("Processed video not found. Detection might have failed.")

# Main Combined Cell - For Colab
from google.colab import files
from google.colab.patches import cv2_imshow
from IPython.display import display, Video
from moviepy.editor import VideoFileClip
from ultralytics import YOLO
import torch
import os
import cv2
import glob
import shutil
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

# Step 1: Upload Button
print("üì§ Please upload your video file")
uploaded = files.upload()
video_path = list(uploaded.keys())[0]

# Step 2: Load YOLOv5
model = YOLO("yolov5s.pt")  # You can use 'yolov8n.pt' too if installed

# Step 3: Run Detection
print("üîç Detecting objects in your video...")
results = model(video_path, save=True)

# Step 4: Find Output Video Path
base = os.path.basename(video_path).rsplit(".", 1)[0]
predict_dir = sorted(glob.glob("runs/detect/predict*"))[-1]
detected_video_path = None
for ext in ['.mp4', '.avi', '.mov']:
    path = os.path.join(predict_dir, base + ext)
    if os.path.exists(path):
        detected_video_path = path
        break

if not detected_video_path:
    print("‚ùå Processed video not found.")
else:
    print(f"‚úÖ Processed video path: {detected_video_path}")
    # Step 5: Display Final Video in Colab
    print("üé• Displaying processed video in Colab...")
    display(Video(detected_video_path, embed=True, html_attributes='controls'))

    # Step 6: Download Processed Video
    print("\nüíæ Downloading processed video to your local machine...")
    files.download(detected_video_path) # This line is added for automatic download

    # Step 7: BLIP Captioning
    print("üìù Generating caption with BLIP...")
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model_blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

    # Capture first frame
    cap = cv2.VideoCapture(detected_video_path)
    success, frame = cap.read()
    cap.release()
    if success:
        img_path = "temp_first_frame.jpg"
        cv2.imwrite(img_path, frame)
        raw_image = Image.open(img_path).convert('RGB')
        inputs = processor(raw_image, return_tensors="pt")
        out = model_blip.generate(**inputs)
        caption = processor.decode(out[0], skip_special_tokens=True)
        print(f"üó£Ô∏è Caption: {caption}")
    else:
        print("‚ùå Could not extract frame for captioning.")

# Step 8: Upload Button at End
print("\n‚¨áÔ∏è Click to re-upload another video anytime:")
files.upload()